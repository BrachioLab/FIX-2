{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36a66e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreyah/miniconda3/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c1229",
   "metadata": {},
   "source": [
    "### Generate Alignment Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0e87471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Less than 100 scores for socratic on cardiac: 3\n",
      "Warning: Less than 100 scores for subq on cardiac: 2\n",
      "-----------------------\n",
      "\\textbf{Vanilla} & 0.42 & 0.83 & 0.63 & 0.64 & 0.30 & 0.52 & 0.54 \\\\\n",
      "\\textbf{Chain-of-Thought} & 0.39 & 0.81 & 0.62 & 0.61 & 0.34 & 0.56 & 0.53 \\\\\n",
      "\\textbf{Socratic Prompting} & 0.41 & 0.80 & 0.60 & 0.62 & 0.37 & 0.50 & 0.54 \\\\\n",
      "\\textbf{SubQ Decomposition} & 0.35 & 0.82 & 0.60 & 0.58 & 0.36 & 0.39 & 0.56 \\\\\n"
     ]
    }
   ],
   "source": [
    "baselines = [\"vanilla\", \"cot\", \"socratic\", \"subq\"]\n",
    "datasets = [\"massmaps\", \"supernova\", \"politeness\", \"emotion\", \"cholec\", \"cardiac\", \"sepsis\"]\n",
    "models = [\"gpt-4o\"]\n",
    "\n",
    "# Map for pretty LaTeX-style formatting of baselines\n",
    "baseline_names = {\n",
    "    \"vanilla\": \"\\\\textbf{Vanilla}\",\n",
    "    \"cot\": \"\\\\textbf{Chain-of-Thought}\",\n",
    "    \"socratic\": \"\\\\textbf{Socratic Prompting}\",\n",
    "    \"subq\": \"\\\\textbf{SubQ Decomposition}\"\n",
    "}\n",
    "\n",
    "# Collect scores\n",
    "results = defaultdict(list)\n",
    "\n",
    "for model in models:\n",
    "    for baseline in baselines:\n",
    "        for dataset in datasets:\n",
    "            path = os.path.join(baseline, f\"{dataset}_{model}.json\")\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"File not found: {path}\")\n",
    "                continue\n",
    "            with open(path, 'r') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    scores = [entry[\"final_alignment_score\"] for entry in data if \"final_alignment_score\" in entry and entry[\"final_alignment_score\"] is not None]\n",
    "                    if scores:\n",
    "                        avg = sum(scores) / len(scores)\n",
    "                        results[(baseline, dataset)] = avg\n",
    "                    #print length of scores\n",
    "                    if(len(scores) < 100):\n",
    "                        print(f\"Warning: Less than 100 scores for {baseline} on {dataset}: {len(scores)}\")\n",
    "                except Exception as e:\n",
    "                    print(scores)\n",
    "                    print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "    # Generate LaTeX rows\n",
    "    print(\"-----------------------\")\n",
    "    for baseline in baselines:\n",
    "        row = [baseline_names[baseline]]\n",
    "        for dataset in datasets:\n",
    "            avg_score = results.get((baseline, dataset), \"\")\n",
    "            if isinstance(avg_score, float):\n",
    "                row.append(f\"{avg_score:.2f}\")\n",
    "            else:\n",
    "                row.append(\"\")  # Leave blank if not available\n",
    "                print(f\"Warning: No score found for {baseline} on {dataset}\")\n",
    "        print(\" & \".join(row) + \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3f9d6",
   "metadata": {},
   "source": [
    "### Generate Accuracy Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f13ad80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Less than 100 scores for socratic on cardiac: 3\n",
      "Neither accuracy nor mse found in subq/cardiac_gpt-4o.json\n",
      "Warning: Less than 100 scores for subq on cardiac: 0\n",
      "Generating LaTeX table rows:\n",
      "\n",
      "\\textbf{Vanilla} & 0.04 & 0.10 & 0.92 & 0.26 & 0.07 & 0.57 & 0.66 \\\\\n",
      "\\textbf{Chain-of-Thought} & 0.04 & 0.09 & 0.82 & 0.27 & 0.10 & 0.46 & 0.71 \\\\\n",
      "\\textbf{Socratic Prompting} & 0.04 & 0.13 & 0.83 & 0.29 & 0.11 & 0.00 & 0.66 \\\\\n",
      "Warning: No score found for subq on cardiac\n",
      "\\textbf{SubQ Decomposition} & 0.05 & 0.12 & 0.84 & 0.29 & 0.12 &  & 0.66 \\\\\n"
     ]
    }
   ],
   "source": [
    "baselines = [\"vanilla\", \"cot\", \"socratic\", \"subq\"]\n",
    "datasets = [\"massmaps\", \"supernova\", \"politeness\", \"emotion\", \"cholec\", \"cardiac\", \"sepsis\"]\n",
    "models = [\"gpt-4o\"]\n",
    "\n",
    "# Collect scores\n",
    "results = defaultdict(list)\n",
    "\n",
    "for model in models:\n",
    "    for baseline in baselines:\n",
    "        for dataset in datasets:\n",
    "            path = os.path.join(baseline, f\"{dataset}_{model}.json\")\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"File not found: {path}\")\n",
    "                continue\n",
    "            with open(path, 'r') as f:\n",
    "                try:\n",
    "                    accuracy = []\n",
    "                    data = json.load(f)\n",
    "                    if \"accuracy\" in data[0]:\n",
    "                        accuracy = [entry[\"accuracy\"] for entry in data if entry[\"accuracy\"] is not None]   \n",
    "                    elif \"mse\" in data[0]:\n",
    "                        accuracy = [entry[\"mse\"] for entry in data if entry[\"mse\"] is not None]   \n",
    "                    elif \"mse_loss\" in data[0]:\n",
    "                        accuracy = [(entry[\"mse_loss\"][\"Omega_m\"] + entry[\"mse_loss\"][\"sigma_8\"])/2 for entry in data if entry[\"mse_loss\"] is not None]\n",
    "                    elif \"safe_iou\" in data[0]:\n",
    "                        accuracy = [(entry[\"safe_iou\"] + entry[\"unsafe_iou\"])/2 for entry in data if entry[\"safe_iou\"] is not None]\n",
    "                    else:\n",
    "                        print(f\"Neither accuracy nor mse found in {path}\")           \n",
    "                    if accuracy:\n",
    "                        avg = sum(accuracy) / len(accuracy)\n",
    "                        results[(baseline, dataset)] = avg\n",
    "                    if(len(accuracy) < 100):\n",
    "                        print(f\"Warning: Less than 100 scores for {baseline} on {dataset}: {len(accuracy)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "# Map for pretty LaTeX-style formatting of baselines\n",
    "baseline_names = {\n",
    "    \"vanilla\": \"\\\\textbf{Vanilla}\",\n",
    "    \"cot\": \"\\\\textbf{Chain-of-Thought}\",\n",
    "    \"socratic\": \"\\\\textbf{Socratic Prompting}\",\n",
    "    \"subq\": \"\\\\textbf{SubQ Decomposition}\"\n",
    "}\n",
    "\n",
    "# Generate LaTeX rows\n",
    "print(\"Generating LaTeX table rows:\\n\")\n",
    "for baseline in baselines:\n",
    "    row = [baseline_names[baseline]]\n",
    "    for dataset in datasets:\n",
    "        avg_score = results.get((baseline, dataset), \"\")\n",
    "        if isinstance(avg_score, float):\n",
    "            row.append(f\"{avg_score:.2f}\")\n",
    "        else:\n",
    "            row.append(\"\")  # Leave blank if not available\n",
    "            print(f\"Warning: No score found for {baseline} on {dataset}\")\n",
    "    print(\" & \".join(row) + \" \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3410c7",
   "metadata": {},
   "source": [
    "### Criteria Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d3ac57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts for subq on massmaps:\n",
      "Connectivity of the Cosmic Web: 123\n",
      "Density Contrast Extremes: 931\n",
      "Filament Thickness and Sharpness: 17\n",
      "Fine-Scale Clumpiness: 316\n",
      "Lensing Peak (Cluster) Abundance: 361\n",
      "Void Size and Frequency: 192\n",
      "\n",
      "Counts for subq on supernova:\n",
      "Characteristic rise-and-decline rates—such as the fast-rise/slow-fade morphology of many supernovae—encode energy-release physics and serve as strong class discriminators.: 2\n",
      "Characteristic rise-and-decline rates—such as the fast‑rise/slow‑fade morphology of many supernovae—encode energy‑release physics and serve as strong class discriminators.: 1\n",
      "Characteristic rise‑and‑decline rates—such as the fast‑rise/slow‑fade morphology of many supernovae—encode energy‑release physics and serve as strong class discriminators.: 295\n",
      "Contiguous non-zero flux segments confirm genuine astrophysical activity and define the time windows from which transient features should be extracted.: 7\n",
      "Contiguous non‑zero flux segments confirm genuine astrophysical activity and define the time windows from which transient features should be extracted.: 108\n",
      "Eclipsing binary (EB): 1\n",
      "Filter-specific secondary maxima or shoulders in red/near-IR bands—prominent in SNe Ia—are morphological features absent in most core-collapse SNe.: 1\n",
      "Filter-specific secondary maxima or shoulders in red/near‑IR bands—prominent in SNe Ia—are morphological features absent in most core-collapse SNe.: 2\n",
      "Filter-specific secondary maxima or shoulders in red/near‑IR bands—prominent in SNe Ia—are morphological features absent in most core‑collapse SNe.: 2\n",
      "Filter‑specific secondary maxima or shoulders in red/near‑IR bands—prominent in SNe Ia—are morphological features absent in most core‑collapse SNe.: 3\n",
      "Filter‑specific secondary maxima or shoulders in red/near‑IR bands—prominent in SNe Ia—are morphological features absent in most core‑collapse SNe.: 80\n",
      "Locally smooth, monotonic flux trends across one or multiple bands (plateaus, linear decays) capture physical evolution stages and help distinguish SN II-P, SN II-L, and related classes.: 8\n",
      "Locally smooth, monotonic flux trends across one or multiple bands (plateaus, linear decays) capture physical evolution stages and help distinguish SN II‑P, SN II‑L, and related classes.: 1\n",
      "Locally smooth, monotonic flux trends across one or multiple bands (plateaus, linear decays) capture physical evolution stages and help distinguish SN II-P, SN II-L, and related classes.: 1\n",
      "Locally smooth, monotonic flux trends across one or multiple bands (plateaus, linear decays) capture physical evolution stages and help distinguish SN II‑P, SN II‑L, and related classes.: 246\n",
      "Microlens-single or single-lens gravitational lensing.: 1\n",
      "Peak-to-trough photometric amplitude separates high-energy explosive events (multi-magnitude outbursts) from low-amplitude periodic or stochastic variables.: 7\n",
      "Peak‑to‑trough photometric amplitude separates high‑energy explosive events (multi‑magnitude outbursts) from low‑amplitude periodic or stochastic variables.: 191\n",
      "Periodic light curves with stable periods and distinctive Fourier amplitude- and phase-ratios (e.g., φ21, φ31) flag pulsators and eclipsing binaries rather than one-off transients.: 2\n",
      "Periodic light curves with stable periods and distinctive Fourier amplitude‑ and phase‑ratios (e.g., φ21, φ31) flag pulsators and eclipsing binaries rather than one-off transients.: 8\n",
      "Periodic light curves with stable periods and distinctive Fourier amplitude‑ and phase‑ratios (e.g., φ21, φ31) flag pulsators and eclipsing binaries rather than one‑off transients.: 1458\n",
      "Periodic light curves with stable periods and distinctive Fourier amplitude– and phase–ratios (e.g., φ21, φ31) flag pulsators and eclipsing binaries rather than one‑off transients.: 1\n",
      "Total event duration, measured from first detection to return to baseline, distinguishes short-lived kilonovae and superluminous SNe from longer plateau or AGN variability phases.: 9\n",
      "Total event duration, measured from first detection to return to baseline, distinguishes short‑lived kilonovae and superluminous SNe from longer plateau or AGN variability phases.: 92\n",
      "\n",
      "Counts for subq on politeness:\n",
      "Apologies and Acknowledgment of Fault: 88\n",
      "Avoidance of Profanity or Negative Emotion: 390\n",
      "Bluntness and Direct Commands: 260\n",
      "Compliments and Praise: 48\n",
      "Courteous Politeness Markers: 186\n",
      "Discourse Management with Markers: 120\n",
      "Empathy or Emotional Support: 132\n",
      "First-Person Subjectivity Markers: 97\n",
      "Gratitude Expressions: 167\n",
      "Greeting and Interaction Initiation: 29\n",
      "Hedging and Tentative Language: 238\n",
      "Honorifics and Formal Address: 149\n",
      "Inclusive Pronouns and Group-Oriented Phrasing: 19\n",
      "Indirect and Modal Requests: 125\n",
      "Ingroup Language and Informality: 26\n",
      "Questions as Indirect Strategies: 112\n",
      "Second Person Responsibility or Engagement: 113\n",
      "Softened Disagreement or Face-Saving Critique: 385\n",
      "Urgency or Immediacy of Language: 20\n",
      "\n",
      "Counts for subq on emotion:\n",
      "Affection & Care Words: 105\n",
      "Arousal: 68\n",
      "Aversion Terms: 59\n",
      "Confusion Phrases: 84\n",
      "Curiosity Questions: 157\n",
      "Emotion Words & Emojis: 106\n",
      "Expressive Punctuation: 11\n",
      "Gratitude Expressions: 22\n",
      "Humor/Laughter Markers: 181\n",
      "Loss or Let-Down Words: 193\n",
      "Other-Blame Statements: 225\n",
      "Praise & Compliments: 23\n",
      "Relief Indicators: 39\n",
      "Self-Blame & Apologies: 49\n",
      "Self-Credit Statements: 57\n",
      "Surprise Exclamations: 64\n",
      "Threat/Worry Language: 122\n",
      "Valence: 474\n",
      "\n",
      "Counts for subq on cholec:\n",
      "1: 229\n",
      "2: 68\n",
      "3: 345\n",
      "4: 572\n",
      "5: 702\n",
      "6: 52\n",
      "7: 1998\n",
      "8: 2\n",
      "9: 100\n",
      "10: 89\n",
      "11: 82\n",
      "Error processing subq/cardiac_gpt-4o.json: alignment_categories not found in subq/cardiac_gpt-4o.json\n",
      "\n",
      "Counts for subq on cardiac:\n",
      "Advanced Age: 178\n",
      "Advanced Age and/or Male Sex: 1\n",
      "Bradycardia or Heart-Rate Drop: 47\n",
      "Critical Illness (Sepsis/Shock): 135\n",
      "Depressed Heart Rate Variability: 272\n",
      "Dynamic ST-Segment Changes: 81\n",
      "Electrical Alternans: 18\n",
      "Extreme Tachyarrhythmias: 278\n",
      "Insufficient Information: 1\n",
      "Male Sex: 6\n",
      "Multiple Potential Categories (Extreme Tachyarrhythmias, Ventricular Ectopy/NSVT, Depressed Heart Rate Variability, etc.): 1\n",
      "No Specific Category: 1\n",
      "None: 1\n",
      "None explicitly applicable: 1\n",
      "None of the Above: 1\n",
      "None of the categories directly align: 1\n",
      "Prolonged QT Interval: 5\n",
      "QRS Widening (Conduction Delay): 12\n",
      "Severe Hyperkalemia Signs: 21\n",
      "Unable to Determine: 1\n",
      "Unclear: 1\n",
      "Underlying Cardiac Disease: 143\n",
      "Ventricular Ectopy/NSVT: 68\n",
      "\n",
      "Counts for subq on sepsis:\n",
      "A Elevated NEWS Score (≥5 points): 2\n",
      "A category cannot be definitively determined.: 1\n",
      "AElevated NEWS Score (≥5 points): 23\n",
      "Early Antibiotic/Culture Orders (within 2 hours): 230\n",
      "Elderly Susceptibility (Age ≥65 years): 40\n",
      "Elevated NEWS Score (≥5 points): 291\n",
      "Elevated SOFA Score (≥2 points): 1\n",
      "Elevated Serum Lactate (≥2 mmol/L): 206\n",
      "Elevated Serum Lactate (≥2 mmol/L) & SIRS Positivity (≥2 Criteria): 1\n",
      "Elevated Shock Index (≥1.0): 232\n",
      "High qSOFA Score (≥2): 166\n",
      "N/A (No specific category): 1\n",
      "National Early Warning Score (NEWS) of ≥ 5–7: 1\n",
      "No Specific Category: 3\n",
      "No Strong Alignment: 1\n",
      "No direct category alignment: 1\n",
      "No specific category alignment: 1\n",
      "No specific category aligns: 1\n",
      "No specific category aligns closely: 1\n",
      "No specific category aligns well: 1\n",
      "No specific expert category directly applies: 1\n",
      "No strong alignment with a specific category: 1\n",
      "None: 15\n",
      "None (No compelling risk): 1\n",
      "None (No specific category aligns): 1\n",
      "None (or No Specific Category): 1\n",
      "None Applicable: 1\n",
      "None of the Categories: 1\n",
      "None of the Expert Sepsis Categories: 1\n",
      "None of the expert categories directly align with the claim.: 1\n",
      "None of the expert categories explicitly align: 1\n",
      "None of the expert sepsis categories are directly applicable.: 1\n",
      "None of the listed categories: 1\n",
      "None of the listed categories specifically align with elevated BNP NT-PRO levels.: 1\n",
      "None of the provided categories: 3\n",
      "None specifically applicable: 1\n",
      "Not Explicitly Categorized: 1\n",
      "Not applicable: 1\n",
      "Not explicitly aligned with any category: 1\n",
      "Presence of ≥ 2 SIRS criteria: 1\n",
      "SIRS Positivity (≥2 Criteria): 933\n",
      "SOFA Score Increase (≥2 points): 449\n",
      "Sepsis-Associated Hypotension (SBP <90 mmHg or MAP <70 mmHg, or ≥40 mmHg drop): 195\n"
     ]
    }
   ],
   "source": [
    "baselines = [\"vanilla\", \"cot\", \"socratic\", \"subq\"]\n",
    "datasets = [\"massmaps\", \"supernova\", \"politeness\", \"emotion\", \"cholec\", \"cardiac\", \"sepsis\"]\n",
    "# datasets = [\"emotion\", \"sepsis\"]\n",
    "models = [\"gpt-4o\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    categories_all = []\n",
    "    for baseline in baselines:\n",
    "        for model in models: \n",
    "            path = os.path.join(baseline, f\"{dataset}_{model}.json\")\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"File not found: {path}\")\n",
    "                continue\n",
    "            with open(path, 'r') as f:\n",
    "                try:\n",
    "                    categories = []\n",
    "                    data = json.load(f)\n",
    "                    if \"alignment_categories\" in data[0]:\n",
    "                        categories = [entry[\"alignment_categories\"] for entry in data if entry[\"alignment_categories\"] is not None] \n",
    "                    elif \"alignment_category\" in data[0]:\n",
    "                        categories = [entry[\"alignment_category\"] for entry in data if entry[\"alignment_category\"] is not None]\n",
    "                    elif \"aligned_category_ids\" in data[0]:\n",
    "                        categories = [entry[\"aligned_category_ids\"] for entry in data if entry[\"aligned_category_ids\"] is not None]\n",
    "                    else:\n",
    "                        raise ValueError(f\"alignment_categories not found in {path}\") \n",
    "                    for category_list in categories:\n",
    "                        for category_item in category_list:\n",
    "                            categories_all.append(category_item)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {path}: {e}\")           \n",
    "    # Count occurrences of each category\n",
    "    names, counts = np.unique(categories_all, return_counts=True)\n",
    "\n",
    "    print(f\"\\nCounts for {baseline} on {dataset}:\")\n",
    "    for name, count in zip(names, counts):\n",
    "        print(f\"{name}: {count}\")\n",
    "            #matplotlib bar plot\n",
    "            # plt.figure(figsize=(10, 6))\n",
    "            # plt.bar(names, counts)\n",
    "            # plt.xlabel('Categories')\n",
    "            # plt.ylabel('Counts')\n",
    "            # plt.title(f'Category Counts for {baseline} on {dataset}')\n",
    "            # plt.xticks(rotation=45)\n",
    "            # plt.tight_layout()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368505e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
