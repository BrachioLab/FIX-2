{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36a66e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c1229",
   "metadata": {},
   "source": [
    "### Generate Alignment Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0e87471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for vanilla on massmaps: 100\n",
      "Scores for vanilla on supernova: 107\n",
      "Scores for vanilla on politeness: 119\n",
      "Scores for vanilla on emotion: 112\n",
      "Scores for vanilla on cholec: 150\n",
      "Scores for vanilla on cardiac: 104\n",
      "Scores for vanilla on sepsis: 108\n",
      "Scores for cot on massmaps: 100\n",
      "Scores for cot on supernova: 107\n",
      "Scores for cot on politeness: 119\n",
      "Scores for cot on emotion: 112\n",
      "Scores for cot on cholec: 150\n",
      "Scores for cot on cardiac: 124\n",
      "Scores for cot on sepsis: 105\n",
      "Scores for socratic on massmaps: 100\n",
      "Scores for socratic on supernova: 110\n",
      "Scores for socratic on politeness: 120\n",
      "Scores for socratic on emotion: 112\n",
      "Scores for socratic on cholec: 150\n",
      "Scores for socratic on cardiac: 3\n",
      "Scores for socratic on sepsis: 108\n",
      "Scores for subq on massmaps: 100\n",
      "Scores for subq on supernova: 110\n",
      "Scores for subq on politeness: 120\n",
      "Scores for subq on emotion: 2\n",
      "Scores for subq on cholec: 150\n",
      "Scores for subq on cardiac: 2\n",
      "Scores for subq on sepsis: 108\n",
      "Generating LaTeX table rows:\n",
      "\n",
      "\\textbf{Vanilla} & 0.42 & 0.83 & 0.63 & 0.64 & 0.30 & 0.52 & 0.54 \\\\\n",
      "\\textbf{Chain-of-Thought} & 0.39 & 0.81 & 0.62 & 0.61 & 0.34 & 0.56 & 0.53 \\\\\n",
      "\\textbf{Socratic Prompting} & 0.41 & 0.80 & 0.60 & 0.62 & 0.37 & 0.50 & 0.54 \\\\\n",
      "\\textbf{SubQ Decomposition} & 0.35 & 0.82 & 0.60 & 0.29 & 0.36 & 0.39 & 0.56 \\\\\n"
     ]
    }
   ],
   "source": [
    "baselines = [\"vanilla\", \"cot\", \"socratic\", \"subq\"]\n",
    "datasets = [\"massmaps\", \"supernova\", \"politeness\", \"emotion\", \"cholec\", \"cardiac\", \"sepsis\"]\n",
    "models = [\"gpt-4o\"]\n",
    "\n",
    "# Map for pretty LaTeX-style formatting of baselines\n",
    "baseline_names = {\n",
    "    \"vanilla\": \"\\\\textbf{Vanilla}\",\n",
    "    \"cot\": \"\\\\textbf{Chain-of-Thought}\",\n",
    "    \"socratic\": \"\\\\textbf{Socratic Prompting}\",\n",
    "    \"subq\": \"\\\\textbf{SubQ Decomposition}\"\n",
    "}\n",
    "\n",
    "# Collect scores\n",
    "results = defaultdict(list)\n",
    "\n",
    "for model in models:\n",
    "    for baseline in baselines:\n",
    "        for dataset in datasets:\n",
    "            path = os.path.join(baseline, f\"{dataset}_{model}.json\")\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"File not found: {path}\")\n",
    "                continue\n",
    "            with open(path, 'r') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    scores = [entry[\"final_alignment_score\"] for entry in data if \"final_alignment_score\" in entry and entry[\"final_alignment_score\"] is not None]\n",
    "                    if scores:\n",
    "                        avg = sum(scores) / len(scores)\n",
    "                        results[(baseline, dataset)] = avg\n",
    "                    #print length of scores\n",
    "                    print(f\"Scores for {baseline} on {dataset}: {len(scores)}\")\n",
    "                except Exception as e:\n",
    "                    print(scores)\n",
    "                    print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "    # Generate LaTeX rows\n",
    "    print(\"Generating LaTeX table rows:\\n\")\n",
    "    for baseline in baselines:\n",
    "        row = [baseline_names[baseline]]\n",
    "        for dataset in datasets:\n",
    "            avg_score = results.get((baseline, dataset), \"\")\n",
    "            if isinstance(avg_score, float):\n",
    "                row.append(f\"{avg_score:.2f}\")\n",
    "            else:\n",
    "                row.append(\"\")  # Leave blank if not available\n",
    "                print(f\"Warning: No score found for {baseline} on {dataset}\")\n",
    "        print(\" & \".join(row) + \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f13ad80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neither accuracy nor mse found in vanilla/supernova_gpt-4o.json\n",
      "Neither accuracy nor mse found in subq/cardiac_gpt-4o.json\n",
      "Generating LaTeX table rows:\n",
      "\n",
      "\\textbf{Vanilla} & 0.04 & 0.04 & 0.92 & 0.26 & 0.07 & 0.57 & 0.66 \\\\\n",
      "\\textbf{Chain-of-Thought} & 0.04 & 0.09 & 0.82 & 0.27 & 0.10 & 0.46 & 0.71 \\\\\n",
      "\\textbf{Socratic Prompting} & 0.04 & 0.13 & 0.83 & 0.29 & 0.11 & 0.00 & 0.66 \\\\\n",
      "\\textbf{SubQ Decomposition} & 0.05 & 0.12 & 0.84 & 0.00 & 0.12 & 0.12 & 0.66 \\\\\n"
     ]
    }
   ],
   "source": [
    "baselines = [\"vanilla\", \"cot\", \"socratic\", \"subq\"]\n",
    "datasets = [\"massmaps\", \"supernova\", \"politeness\", \"emotion\", \"cholec\", \"cardiac\", \"sepsis\"]\n",
    "models = [\"gpt-4o\"]\n",
    "\n",
    "for model in models:\n",
    "    for baseline in baselines:\n",
    "        for dataset in datasets:\n",
    "            path = os.path.join(baseline, f\"{dataset}_{model}.json\")\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"File not found: {path}\")\n",
    "                continue\n",
    "            with open(path, 'r') as f:\n",
    "                try:\n",
    "                    accuracy = []\n",
    "                    data = json.load(f)\n",
    "                    if \"accuracy\" in data[0]:\n",
    "                        accuracy = [entry[\"accuracy\"] for entry in data if entry[\"accuracy\"] is not None]   \n",
    "                    elif \"mse\" in data[0]:\n",
    "                        accuracy = [entry[\"mse\"] for entry in data if entry[\"mse\"] is not None]   \n",
    "                    elif \"mse_loss\" in data[0]:\n",
    "                        accuracy = [(entry[\"mse_loss\"][\"Omega_m\"] + entry[\"mse_loss\"][\"sigma_8\"])/2 for entry in data if entry[\"mse_loss\"] is not None]\n",
    "                    elif \"safe_iou\" in data[0]:\n",
    "                        accuracy = [(entry[\"safe_iou\"] + entry[\"unsafe_iou\"])/2 for entry in data if entry[\"safe_iou\"] is not None]\n",
    "                    else:\n",
    "                        print(f\"Neither accuracy nor mse found in {path}\")           \n",
    "                    if accuracy:\n",
    "                        avg = sum(accuracy) / len(accuracy)\n",
    "                        results[(baseline, dataset)] = avg\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "# Map for pretty LaTeX-style formatting of baselines\n",
    "baseline_names = {\n",
    "    \"vanilla\": \"\\\\textbf{Vanilla}\",\n",
    "    \"cot\": \"\\\\textbf{Chain-of-Thought}\",\n",
    "    \"socratic\": \"\\\\textbf{Socratic Prompting}\",\n",
    "    \"subq\": \"\\\\textbf{SubQ Decomposition}\"\n",
    "}\n",
    "\n",
    "# Generate LaTeX rows\n",
    "print(\"Generating LaTeX table rows:\\n\")\n",
    "for baseline in baselines:\n",
    "    row = [baseline_names[baseline]]\n",
    "    for dataset in datasets:\n",
    "        avg_score = results.get((baseline, dataset), \"\")\n",
    "        if isinstance(avg_score, float):\n",
    "            row.append(f\"{avg_score:.2f}\")\n",
    "        else:\n",
    "            row.append(\"\")  # Leave blank if not available\n",
    "            print(f\"Warning: No score found for {baseline} on {dataset}\")\n",
    "    print(\" & \".join(row) + \" \\\\\\\\\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
