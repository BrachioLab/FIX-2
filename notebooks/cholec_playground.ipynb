{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a7435e-c79b-4456-ac6d-b767eae24dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonxue/lib/miniconda3/envs/tfl/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/antonxue/lib/miniconda3/envs/tfl/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/antonxue/lib/miniconda3/envs/tfl/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms.functional as tvtf\n",
    "import json\n",
    "\n",
    "import sys; sys.path.append(\"../src\")\n",
    "from prompts.explanations import *\n",
    "from cholec import CholecExample, CholecDataset, load_model, items_to_examples\n",
    "from llms import image_to_base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e4e261-13f4-4909-b451-50ee9b6a7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = load_model(\"claude-3-5-sonnet-latest\")\n",
    "llm = load_model(\"gemini-2.5-pro-exp-03-25\")\n",
    "# llm = load_model(\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd5b2c7-0db8-4ec8-8d12-aa296f7c8aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonxue/lib/miniconda3/envs/tfl/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = CholecDataset(split=\"train\", image_size=(180,320))\n",
    "item = dataset[28]\n",
    "image = item[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc5d23-0f73-4427-8b15-2a0ee92d1300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a0ce52-3223-4db2-8de7-726970c06d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = load_cholec_prompt(\"vanilla\") + (image,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab7d80b0-1bab-4a36-979f-e4619d359ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: (length:  0 )\n",
      "candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.MAX_TOKENS: 'MAX_TOKENS'>, avg_logprobs=None, grounding_metadata=None, index=0, logprobs_result=None, safety_ratings=None)] create_time=None response_id=None model_version='gemini-2.5-pro-exp-03-25' prompt_feedback=None usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=None, candidates_tokens_details=None, prompt_token_count=6453, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=4131), ModalityTokenCount(modality=<MediaModality.IMAGE: 'IMAGE'>, token_count=2322)], thoughts_token_count=4096, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=10549, traffic_type=None) automatic_function_calling_history=[] parsed=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = llm(prompt)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9613c78-9d5f-4377-82ef-feddf8f9deff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f47e75e-2116-4ec1-b6a5-5816896deaae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a28f8-9479-480a-b8a2-2d336c67738c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729296e7-07c1-46dc-9653-998a00caf55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm((\"hello? can you describe this image please:\", image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711e095-ab5d-44fc-93f1-c6b935d2a124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f5698-bf58-441f-98eb-b2e2855df31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CholecDataset(split=\"train\", image_size=(180,320))\n",
    "item = dataset[28]\n",
    "image = item[\"image\"]\n",
    "\n",
    "# items = []\n",
    "# for i, item in enumerate(dataset):\n",
    "#     if i % 5 != 0:\n",
    "#         continue\n",
    "\n",
    "#     if (item[\"gonogo\"] == 1).float().mean() > 0.05 and (item[\"gonogo\"] == 2).float().mean() > 0.05:\n",
    "#         items.append(item)\n",
    "\n",
    "#     if len(items) >= 10:\n",
    "#         break\n",
    "\n",
    "# image = dataset[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3a432-bc4d-4abe-88be-7dd761a32ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_base64(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc59929-5907-4148-88fc-86d7571f5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvtf.to_pil_image(image, mode=\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141103f1-9f34-4a58-aa38-feed2b82106f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1a511-c599-43c1-b7ec-6ff77b45e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cholec_vanilla = load_cholec_prompt(\"vanilla\")\n",
    "cholec_vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e010b-e89d-4812-b958-11ae33b93bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = [cholec_vanilla + (i,) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1b9af-3f54-4677-ad85-625955b5db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252e29c-7e6f-4bc6-8f89-920970e1a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gonogo = item[\"gonogo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1091a4-7beb-4b23-aff6-29e33bc5994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gonogo.shape, gonogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff28aff3-5ebf-46fe-b480-2077eafabb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvtf.to_pil_image(gonogo.float(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010fe6f3-9c1e-4bb8-b70a-909fd549bf68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d86a41-a1e2-4dcb-82c0-cc031868cf85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0cd575-4abc-4c83-922d-bca668e1c293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dfed4d-abfd-4e0b-a387-fb2000108d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvtf.to_pil_image(image, mode=\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c09d5-ef9f-436d-91e8-6b7ac24cdf98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71687c0a-015b-47dd-87bc-00f708922c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = llm(cholec_vanilla + (image,))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209dcdf-ce51-4e5c-b3fd-21be93024acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ff9d8-df4c-4d92-b310-98024494b8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38041a52-92d2-4274-8b95-f45ee01dc461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d55b25-3feb-4158-84c0-9d127150f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CholecDataset(split=\"train\", image_size=(180,320))\n",
    "items = []\n",
    "for i, item in enumerate(dataset):\n",
    "    if i % 5 != 0:\n",
    "        continue\n",
    "\n",
    "    if (item[\"gonogo\"] == 1).float().mean() > 0.05 and (item[\"gonogo\"] == 2).float().mean() > 0.05:\n",
    "        items.append(item)\n",
    "\n",
    "    if len(items) >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18969342-9bd3-4a42-a4f8-c6dac5f00815",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_header = \"\"\"\n",
    "You are an expert surgeon with extensive experience in laparoscopic cholecystectomy.\n",
    "You have deep knowledge of anatomy, surgical techniques, and potential complications.\n",
    "Your task is to provide an explanation of where it is safe and unsafe to cut based on an image and a list of numbers.\n",
    "These numbers correspond to a discretization of the image into a 9-high x 16-wide grid.\n",
    "- The top-left corner is index 0.\n",
    "- The top-right corner is index 15.\n",
    "- The bottom-left corner is index 128.\n",
    "- The bottom-right corner is index 143.\n",
    "\n",
    "I will give you an example.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d5739-1a78-483b-9c09-f56d5ae666ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce35779-759f-46f9-80c2-a76002fa9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(items):\n",
    "    image = item[\"image\"]\n",
    "    safe = (item[\"gonogo\"] == 1).long()\n",
    "    unsafe = (item[\"gonogo\"] == 2).long()\n",
    "\n",
    "    a2d = nn.AvgPool2d(kernel_size=20, stride=20, padding=0)\n",
    "    safe_rough = (a2d(safe.float()).squeeze() > 0.1).long()\n",
    "    unsafe_rough = (a2d(unsafe.float()).squeeze() > 0.1).long()\n",
    "\n",
    "    safe_list = safe_rough.view(-1).nonzero().view(-1).tolist()\n",
    "    unsafe_list = unsafe_rough.view(-1).nonzero().view(-1).tolist()\n",
    "    \n",
    "    safe_rough_big = F.interpolate(safe_rough.float().view(1,1,*safe_rough.shape), scale_factor=20).squeeze()\n",
    "    unsafe_rough_big = F.interpolate(unsafe_rough.float().view(1,1,*safe_rough.shape), scale_factor=20).squeeze()\n",
    "\n",
    "    explanation = llm((\n",
    "        prompt_header,\n",
    "        \"Here is the surgery image\", image,\n",
    "        \"Here are the grid indices of the safe region: \" + str(safe_list),\n",
    "        \"These grid indices correspond to the following mask: \", safe_rough_big,\n",
    "        \"Here are the grid indices of the unsafe region: \" + str(unsafe_list),\n",
    "        \"These grid indices correspond to the following mask: \", unsafe_rough_big,\n",
    "        \"\"\"Please give me a 300-500 word description of both regions using the image.\n",
    "        Do not refer to the mask or given indices.\n",
    "        Your job is to generate an explanation that will be used as part of an LLM prompt.\n",
    "        That LLM will only be able to see the original surgery image.\n",
    "        Be concise in your output. Identify only the safe and unsafe regions.\n",
    "        Do not output broadly general surgical advice.\n",
    "        \"\"\"\n",
    "    ))\n",
    "\n",
    "    # explanation = \"\"\n",
    "    \n",
    "    path_prefix = f\"cholec_fewshot_{i}\"\n",
    "    with open(f\"_dump/{path_prefix}_data.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"explanation\": explanation,\n",
    "            \"safe\": safe_list,\n",
    "            \"unsafe\": unsafe_list,\n",
    "        }, f, indent=4)\n",
    "\n",
    "    save_image(image, f\"_dump/{path_prefix}_image.png\")\n",
    "    save_image(safe_rough_big, f\"_dump/{path_prefix}_safe.png\")\n",
    "    save_image(unsafe_rough_big, f\"_dump/{path_prefix}_unsafe.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee46470-0243-4d81-9add-cd8119664d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Safe\", safe_list)\n",
    "print(\"Unsafe\", unsafe_list)\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd0f7d-5006-4181-898f-1cc494502fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig, ax = plt.subplots(2,2)\n",
    "ax[0,0].imshow(safe_rough_big.squeeze().numpy(), cmap=\"gray\")\n",
    "ax[1,0].imshow(safe.squeeze().numpy(), cmap=\"gray\")\n",
    "ax[0,1].imshow(unsafe_rough_big.squeeze().numpy(), cmap=\"gray\")\n",
    "ax[1,1].imshow(unsafe.squeeze().numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85fb81-2614-4028-8463-2906154f7810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9fd18-037b-4312-bf48-2c8754295e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1,) + (5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d3839-3698-476b-a4b7-25fc353215c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
