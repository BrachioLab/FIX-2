{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e000caa4-0301-402f-916e-86f9a1fec6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from diskcache import Cache\n",
    "cache = Cache(\"/shared_data0/llm_cachedir\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c78546-f9b8-46c0-8013-54e2cc22d4ab",
   "metadata": {},
   "source": [
    "### OpenAI Querying Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b59a48e0-4760-43e9-83a9-391199c4ac63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@cache.memoize()\n",
    "def query_openai(prompt, model=\"gpt-4o\"):\n",
    "    with open(\"../API_KEY.txt\", \"r\") as file:\n",
    "        api_key = file.read()\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    num_tries = 0\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            translation = client.chat.completions.create(\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }],\n",
    "                model=model,\n",
    "            )\n",
    "            return translation.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            num_tries += 1\n",
    "            print(\"Try {}; Error: {}\".format(str(num_tries), str(e)))     \n",
    "            time.sleep(3)\n",
    "    return \"ERROR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699b4b7-7179-415c-b94c-63705eb27214",
   "metadata": {},
   "source": [
    "### Load Emotion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6604f849-5d96-4f2d-bf97-7edabe0163ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_data =  load_dataset(\"BrachioLab/emotion\")\n",
    "emotion_data = emotion_data['train'].to_pandas()\n",
    "emotion_data = emotion_data.sample(20, random_state=11).reset_index(drop=True)\n",
    "\n",
    "class EmotionExample:\n",
    "    def __init__(self, text, ground_truth, llm_label, llm_explanation):\n",
    "        self.text = text\n",
    "        self.ground_truth = ground_truth\n",
    "        self.llm_label = llm_label\n",
    "        self.llm_explanation = llm_explanation\n",
    "        self.claims = []\n",
    "        self.relevant_claims = []\n",
    "        self.alignment_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f92c7cc5-8eb8-4f15-8be2-a161cb529fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_labels = {\n",
    "    0: \"admiration\",\n",
    "    1: \"amusement\",\n",
    "    2: \"anger\",\n",
    "    3: \"annoyance\",\n",
    "    4: \"approval\",\n",
    "    5: \"caring\",\n",
    "    6: \"confusion\",\n",
    "    7: \"curiosity\",\n",
    "    8: \"desire\",\n",
    "    9: \"disappointment\",\n",
    "    10: \"disapproval\",\n",
    "    11: \"disgust\",\n",
    "    12: \"embarrassment\",\n",
    "    13: \"excitement\",\n",
    "    14: \"fear\",\n",
    "    15: \"gratitude\",\n",
    "    16: \"grief\",\n",
    "    17: \"joy\",\n",
    "    18: \"love\",\n",
    "    19: \"nervousness\",\n",
    "    20: \"optimism\",\n",
    "    21: \"pride\",\n",
    "    22: \"realization\",\n",
    "    23: \"relief\",\n",
    "    24: \"remorse\",\n",
    "    25: \"sadness\",\n",
    "    26: \"surprise\",\n",
    "    27: \"neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d8e72-555a-4704-b64a-e4ee3e33f1db",
   "metadata": {},
   "source": [
    "### Stage 0: Get LLM Explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3029a222-7398-4eeb-b31c-0d3724b15796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:19,  1.02it/s] \n"
     ]
    }
   ],
   "source": [
    "explanation_prompt = \"\"\"What is the emotion of the following text? Here are the possible labels you could use:\n",
    "admiration\n",
    "amusement\n",
    "anger\n",
    "annoyance\n",
    "approval\n",
    "caring\n",
    "confusion\n",
    "curiosity\n",
    "desire\n",
    "disappointment\n",
    "disapproval\n",
    "disgust\n",
    "embarrassment\n",
    "excitement\n",
    "fear\n",
    "gratitude\n",
    "grief\n",
    "joy\n",
    "love\n",
    "nervousness\n",
    "optimism\n",
    "pride\n",
    "realization\n",
    "relief\n",
    "remorse\n",
    "sadness\n",
    "surprise\n",
    "neutral\n",
    "\n",
    "In addition, provide a paragraph explaining why you gave the text that classification label. Your response should be 2 lines, formatted as follows:\n",
    "Label: <label>\n",
    "Explanation: <explanation>\n",
    "\n",
    "Here is the following text.\n",
    "Text: {}\n",
    "\"\"\"\n",
    "def get_llm_generated_answer(text: str):\n",
    "    prompt = explanation_prompt.format(text)\n",
    "    response = query_openai(prompt)\n",
    "    if response == \"ERROR\":\n",
    "        print(\"Error in querying OpenAI API\")\n",
    "        return None\n",
    "    response_split = [e for e in response.split(\"\\n\") if (e != '' and e.split()[0] in ['Label:', 'Explanation:'])]\n",
    "    llm_label = response_split[0].split(\"Label: \")[1].strip()\n",
    "    explanation = response_split[1].split(\"Explanation: \")[1].strip()\n",
    "    return llm_label, explanation\n",
    "\n",
    "emotion_examples = []\n",
    "for idx,row in tqdm(emotion_data.iterrows()):\n",
    "    llm_label, explanation = get_llm_generated_answer(row['text'])\n",
    "    if llm_label is None:\n",
    "        continue\n",
    "    emotion_examples.append(EmotionExample(\n",
    "        text=row['text'],\n",
    "        ground_truth=emotion_labels[row['labels'][0]],\n",
    "        llm_label=llm_label,\n",
    "        llm_explanation=explanation\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8f333bf-5eb5-4b83-8224-b0c5f1dd7a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amusement',\n",
       " 'amusement',\n",
       " 'joy',\n",
       " 'approval',\n",
       " 'amusement',\n",
       " 'gratitude',\n",
       " 'understanding',\n",
       " 'disapproval',\n",
       " 'amusement',\n",
       " 'curiosity',\n",
       " 'gratitude',\n",
       " 'approval',\n",
       " 'disapproval',\n",
       " 'curiosity',\n",
       " 'embarrassment',\n",
       " 'caring',\n",
       " 'curiosity',\n",
       " 'annoyance',\n",
       " 'surprise',\n",
       " 'urgency']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[emotion_examples[i].llm_label for i in range(len(emotion_examples))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0edc98b2-cb9a-47fe-b400-f2ed30641ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_examples[0].ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7878a2b4-6af5-4028-aff4-89d0f87984da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Creepy Yoda voicepack tbh'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_examples[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3218ee-652f-403b-a162-51e8ca032aae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amusement'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_examples[0].llm_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9463a05e-9561-4bde-9ef6-939bd73b4546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The use of \"Creepy Yoda voicepack tbh\" suggests a tongue-in-cheek or playful reaction, as \"tbh\" (to be honest) paired with \"creepy\" implies a light-hearted observation rather than fear or disgust.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_examples[0].llm_explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34cd288-4c9b-4156-b0ac-e0d4e03d6a83",
   "metadata": {},
   "source": [
    "### Stage 1: Atomic claim extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37fb897d-892a-428f-bc34-f06ad30f60c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "claim_prompt = \"\"\"\n",
    "You will be given a paragraph that explains why a emotion was attributed to an utterance. Your task is to decompose this explanation into individual claims that are:\n",
    "\n",
    "Atomic: Each claim should express only one clear idea or judgment.\n",
    "Standalone: Each claim should be self-contained and understandable without needing to refer back to the paragraph.\n",
    "Faithful: The claims must preserve the original meaning, nuance, and tone. Do not omit hedging language (e.g., \"seems to,\" \"somewhat,\" \"lacks overt markers\") or subjective phrasing if present.\n",
    "\n",
    "Format your output as a list of claims separated by new lines. Do not include any additional text or explanations.\n",
    "\n",
    "Here is an example of how to format your output:\n",
    "\n",
    "INPUT: The use of \"creepy\" and \"tbh\" suggests a negative reaction or discomfort, likely indicating the speaker finds the voicepack irritating or unsettling.\n",
    "\n",
    "OUTPUT:\n",
    "\n",
    "The use of the word \"creepy\" suggests a negative reaction or discomfort.\n",
    "The word \"tbh\" likely indicates that the speaker finds the voicepack irritating or unsettling.\n",
    "\n",
    "Now decompose the following pararaph into atomic, standalone claims:\n",
    "INPUT: {}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def isolate_individual_features(explanation: str):\n",
    "    prompt = claim_prompt.format(explanation)\n",
    "    response = query_openai(prompt)\n",
    "    if response == \"ERROR\":\n",
    "        print(\"Error in querying OpenAI API\")\n",
    "        return None\n",
    "    response = response.replace(\"OUTPUT:\", \"\").strip()\n",
    "    claims = response.split(\"\\n\")\n",
    "    return claims\n",
    "\n",
    "for example in emotion_examples:\n",
    "    claims = isolate_individual_features(example.llm_explanation)\n",
    "    if claims is None:\n",
    "        continue\n",
    "    example.claims = [claim.strip() for claim in claims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6d39203-b482-4673-87e0-f01463281c32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The phrase \"Creepy Yoda voicepack tbh\" suggests a tongue-in-cheek or playful reaction.',\n",
       " 'The word \"tbh\" (to be honest) paired with \"creepy\" implies a light-hearted observation rather than fear or disgust.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_examples[0].claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a155476e-ea49-44e1-98b7-51325075ebe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevance_prompt = \"\"\"You will be given a text, its emotion label, and a claim that may or may not be relevant to an explanation of the emotion label. Your task is to decide whether the claim is relevant to explaining the emotion label for this specific text.\n",
    "\n",
    "A claim is relevant if and only if:\n",
    "(1) It is supported by the content of the utterance (i.e., it does not hallucinate or speculate beyond what is said).\n",
    "(2) It helps explain why the utterance received the given emotion label (i.e., it directly relates to tone, phrasing, or other aspects relevant to the label).\n",
    "\n",
    "Return your answer as:\n",
    "Relevance: <Yes/No>\n",
    "Reasoning: <A brief explanation of your judgment, pointing to specific support or lack thereof>\n",
    "\n",
    "Now, determine whether the following claim is relevant to the given text and emotion label:\n",
    "Text: {}\n",
    "Emotion Label: {}\n",
    "Claim: {}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62012051-1ae8-4420-8cd2-3169b6a10a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Creepy Yoda voicepack tbh\n",
      "Emotion Label: amusement\n",
      "Claim: The phrase \"Creepy Yoda voicepack tbh\" suggests a tongue-in-cheek or playful reaction.\n",
      "Relevance: Yes  \n",
      "Reasoning: The claim directly relates to and is supported by the content of the text. The use of \"Creepy Yoda voicepack tbh\" can indeed suggest a tongue-in-cheek or playful reaction, which aligns well with the emotion label of amusement. The choice of words implies a light-hearted or humorous tone, satisfying both conditions for relevance.\n"
     ]
    }
   ],
   "source": [
    "prompt = relevance_prompt.format(emotion_examples[0].text, emotion_examples[0].llm_label, emotion_examples[0].claims[0])\n",
    "response = query_openai(prompt)\n",
    "print(\"Text:\", emotion_examples[0].text)\n",
    "print(\"Emotion Label:\", emotion_examples[0].llm_label)\n",
    "print(\"Claim:\", emotion_examples[0].claims[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "380a5653-d118-498e-89ea-8cb001e56f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This feels more like r/fellowkids to me. This is nothing compared to some of the more dystopian stuff the government has pushed out to be fair.\n",
      "Emotion Label: disapproval\n",
      "Claim: The text expresses a sense of disapproval or criticism.\n",
      "Relevance: Yes  \n",
      "Reasoning: The claim is relevant because it is supported by the content of the text. The text expresses criticism by comparing the subject to \"r/fellowkids,\" which is often used to criticize attempts at trying too hard to appeal to younger audiences. Additionally, stating that this is \"nothing compared to some of the more dystopian stuff the government has pushed out\" implies a critical or disapproving tone about the subject, which aligns with the emotion label of disapproval.\n"
     ]
    }
   ],
   "source": [
    "prompt = relevance_prompt.format(emotion_examples[7].text, emotion_examples[7].llm_label, emotion_examples[7].claims[0])\n",
    "response = query_openai(prompt)\n",
    "print(\"Text:\", emotion_examples[7].text)\n",
    "print(\"Emotion Label:\", emotion_examples[7].llm_label)\n",
    "print(\"Claim:\", emotion_examples[7].claims[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e93a9-6ecb-4298-8e6f-f1eeecdfbdfa",
   "metadata": {},
   "source": [
    "### Stage 2: Distill relevant claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55d52a4d-327f-4e41-a29c-9277e4b45b95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 11.82it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 966.21it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 991.74it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 982.43it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 953.38it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 758.51it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 978.15it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 285.96it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 169.65it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 943.07it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 957.82it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 973.23it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 682.59it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 851.46it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 106.31it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 841.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 944.66it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 309.27it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 958.19it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 959.50it/s]\n"
     ]
    }
   ],
   "source": [
    "relevance_prompt = \"\"\"You will be given a text, its emotion label, and a claim that may or may not be relevant to an explanation of the emotion label. Your task is to decide whether the claim is relevant to explaining the emotion label for this specific text.\n",
    "\n",
    "A claim is relevant if and only if:\n",
    "(1) It is supported by the content of the text (i.e., it does not hallucinate or speculate beyond what is said).\n",
    "(2) It helps explain why the text received the given emotion label (i.e., it directly relates to tone, phrasing, or other aspects relevant to the label).\n",
    "\n",
    "Return your answer as:\n",
    "Relevance: <Yes/No>\n",
    "Reasoning: <A brief explanation of your judgment, pointing to specific support or lack thereof>\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "[Example 1]\n",
    "Text: Creepy Yoda voicepack tbh\n",
    "Emotion Label: disapproval\n",
    "Claim: The use of the word \"creepy\" paired with \"Yoda voicepack\" suggests a playful tone.\n",
    "Relevance: No  \n",
    "Reasoning: The claim is not relevant because it does not contribute to the emotion label of disapproval. \n",
    "\n",
    "[Example 2]\n",
    "Text: Motor Kombat was awesome. Goofy as hell, but awesome.\n",
    "Emotion Label: amusement\n",
    "Claim: There is an entertaining sentiment typically associated with amusement.\n",
    "Relevance: Yes  \n",
    "Reasoning: The claim is relevant because it accurately captures the tone of the text. The description of \"Motor Kombat\" as \"awesome\" and \"goofy as hell\" suggests an enjoyable and entertaining experience, which aligns with the entertaining sentiment associated with amusement. The text's phrasing supports the emotion label of amusement.\n",
    "\n",
    "[Example 3]\n",
    "Text: And they lived happily ever after.\n",
    "Emotion Label: joy\n",
    "Claim: The phrase typically signifies a positive and fulfilling conclusion to a story.\n",
    "Relevance: Yes\n",
    "Reasoning: The claim is relevant because the phrase \"And they lived happily ever after\" is commonly associated with fairy tales and stories that end on a positive, satisfying note. This association supports the emotion label of amusement by indicating a light-hearted and cheerful conclusion, which is often found in amusing and entertaining stories. The claim does not speculate beyond what is expressed in the text, as the phrase itself directly suggests a positive ending, aligning with feelings of amusement.\n",
    "\n",
    "Now, determine whether the following claim is relevant to the given text and emotion label:\n",
    "Text: {}\n",
    "Emotion Label: {}\n",
    "Claim: {}\n",
    "\"\"\"\n",
    "\n",
    "def is_claim_relevant(text: str, rating: str, claim: str):\n",
    "    prompt = relevance_prompt.format(text, rating, claim)\n",
    "    response = query_openai(prompt)\n",
    "    if response == \"ERROR\":\n",
    "        print(\"Error in querying OpenAI API\")\n",
    "        return None\n",
    "    response = response.replace(\"Relevance:\", \"\").strip()\n",
    "    response = response.split(\"\\n\")\n",
    "    relevance = response[0].strip()\n",
    "    reasoning = response[1].replace(\"Reasoning:\", \"\").strip()\n",
    "    return relevance, reasoning\n",
    "\n",
    "\n",
    "def distill_relevant_features(example: EmotionExample):\n",
    "    relevant_claims = []\n",
    "    for claim in tqdm(example.claims):\n",
    "        relevance, reasoning = is_claim_relevant(example.text, example.llm_label, claim)\n",
    "        if relevance is None:\n",
    "            continue\n",
    "        if relevance == \"Yes\":\n",
    "            relevant_claims.append(claim)\n",
    "    return relevant_claims\n",
    "\n",
    "for example in emotion_examples:\n",
    "    relevant_claims = distill_relevant_features(example)\n",
    "    example.relevant_claims = relevant_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "268de10f-360a-457f-9aec-df58f2b0968e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The phrase \"Creepy Yoda voicepack tbh\" suggests a tongue-in-cheek or playful reaction.',\n",
       " 'The word \"tbh\" (to be honest) paired with \"creepy\" implies a light-hearted observation rather than fear or disgust.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_examples[0].relevant_claims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18500f1d-39d7-44e1-ba49-7c42b19733b9",
   "metadata": {},
   "source": [
    "### Stage 3: Calculate alignment scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1280e3d-ee39-4a0f-8eaa-fca9fe409087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "alignment_prompt = \"\"\"You will be given a text, its emotion label, and a claim that relates to why that label was given. \n",
    "\n",
    "Your task is as follows:\n",
    "1. On a -2 to 2 scale, rate the valence of what is conveyed in the claim. \n",
    "2. On a -2 to 2 scale, rate the arousal of what is conveyed in the claim. \n",
    "\n",
    "Valence is the degree of positivity or negativity expressed in a sentence. It ranges from negative (sadness, displeasure) to positive (happiness, pleasure). -2 = very negative valence and 2 = very positive valence.\n",
    "Arousal is the level of intensity or energy conveyed by the emotion in a sentence. It ranges from low (calm, tired) to high (alarmed, astonished). -2 = very low arousal and 2 = very high arousal.\n",
    "\n",
    "\n",
    "Return your answer as:\n",
    "Valence Rating: <rating>\n",
    "Arousal Rating: <rating>\n",
    "Reasoning: <A brief explanation of why you gave the valence and arousal ratings that you did.>\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "[Example 1]\n",
    "Text: And they lived happily ever after.\n",
    "Emotion Label: joy\n",
    "Claim: The phrase \"And they lived happily ever after\" conveys a sense of joy.\n",
    "Valence Rating: 2\n",
    "Arousal Rating: -1\n",
    "Reasoning: The claim emphasizes the positive emotional resolution implied by the phrase “And they lived happily ever after,” which is strongly associated with joy and satisfaction. However, while the valence is very high due to the happy sentiment, the arousal is relatively low—it suggests contentment and peace rather than excitement or high energy.\n",
    "\n",
    "[Example 2]\n",
    "Text: Motor Kombat was awesome. Goofy as hell, but awesome.\n",
    "Emotion Label: amusement\n",
    "Claim: There is an entertaining sentiment typically associated with amusement.\n",
    "Valence Rating: 2\n",
    "Arousal Rating: 1.5\n",
    "Reasoning: The claim highlights a strongly positive, fun sentiment (“awesome,” “amusement”) indicating high valence. The enthusiastic tone also suggests a relatively high energy or arousal level.\n",
    "\n",
    "[Example 3]\n",
    "Text: This feels more like r/fellowkids to me. This is nothing compared to some of the more dystopian stuff the government has pushed out to be fair.\n",
    "Emotion Label: disapproval\n",
    "Claim: The feeling of the content being forced or out of touch aligns with a sense of disapproval.\n",
    "Valence Rating: -1\n",
    "Arousal Rating: 0.5\n",
    "Reasoning: The claim highlights a negative evaluation—feeling that something is \"forced or out of touch\"—which supports a moderately negative valence. The arousal is slightly above neutral because the tone implies a level of irritation or critical engagement, but it isn’t highly emotional or intense.\n",
    "\n",
    "Now, determine the valence and arousal ratings for the following claim:\n",
    "Text: {}\n",
    "Emotion Label: {}\n",
    "Claim: {}\n",
    "\"\"\"\n",
    "\n",
    "def calculate_expert_alignment_score(text: str, label: str, claim: str):\n",
    "    prompt = alignment_prompt.format(text, label, claim)\n",
    "    response = query_openai(prompt)\n",
    "    if response == \"ERROR\":\n",
    "        print(\"Error in querying OpenAI API\")\n",
    "        return None\n",
    "    response = [e for e in response.strip().split(\"\\n\") if e != \"\"]\n",
    "    valence_rating = float(response[0].replace(\"Valence Rating:\", \"\").strip())\n",
    "    arousal_rating = float(response[1].replace(\"Arousal Rating:\", \"\").strip())\n",
    "    alignment_score = max(abs(valence_rating), abs(arousal_rating))\n",
    "    reasoning = response[2].replace(\"Reasoning:\", \"\").strip()\n",
    "    return valence_rating, arousal_rating, alignment_score, reasoning\n",
    "\n",
    "for example in emotion_examples:\n",
    "    valence_ratings = []\n",
    "    arousal_ratings = []\n",
    "    alignment_scores = []\n",
    "    reasonings = []\n",
    "    for claim in tqdm(example.relevant_claims):\n",
    "        valence_rating, arousal_rating, alignment_score, reasoning = calculate_expert_alignment_score(example.text, example.llm_label, claim)\n",
    "        # if category is None:\n",
    "        #     continue\n",
    "        valence_ratings.append(valence_rating)\n",
    "        arousal_ratings.append(arousal_rating)\n",
    "        alignment_scores.append(alignment_score)\n",
    "        reasonings.append(reasoning)\n",
    "    example.valence_ratings = valence_ratings\n",
    "    example.arousal_ratings = arousal_ratings\n",
    "    example.alignment_scores = alignment_scores\n",
    "    example.reasonings = reasonings\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9d50078-cf85-4773-ae88-a9eb853f49df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5, 1.0]\n",
      "[1.0, 0.5]\n",
      "[1.5, 1.0]\n",
      "['The claim describes an amused reaction, implied by the \"tongue-in-cheek or playful\" response, suggesting a positive emotional undercurrent. The term \"Creepy\" might be conventionally negative, but the context here is playful, resulting in a high positive valence. The arousal is moderately high due to the playful tone, indicating some level of energy and engagement, though it doesn\\'t reach the intensity of a very high-arousal situation.', 'The claim suggests a mildly positive sentiment through the lens of amusement, as indicated by the use of \"tbh\" in a casual and informal manner. The use of \"creepy\" in combination with a light-hearted context like \"Yoda voicepack\" implies a non-threatening, playful kind of creepiness, contributing to a slightly positive valence. The arousal rating reflects the light-hearted, yet mildly engaging nature of the sentiment, but it\\'s not particularly intense or energetic.']\n"
     ]
    }
   ],
   "source": [
    "print(emotion_examples[0].valence_ratings)\n",
    "print(emotion_examples[0].arousal_ratings)\n",
    "print(emotion_examples[0].alignment_scores)\n",
    "print(emotion_examples[0].reasonings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ebca1-6683-4462-a473-9961a545c519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
