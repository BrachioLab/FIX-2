{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "574ef81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04b79af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textbf{Mass Maps} & 66 & 48 & 0.900 & 0.826 & 0.979 & 0.4059 \\\\\n",
      "\\textbf{Supernova} & 74 & 62 & 0.950 & 0.892 & 0.903 & 0.4946 \\\\\n",
      "\\textbf{Politeness} & 72 & 58 & 0.950 & 0.931 & 0.914 & 0.6604 \\\\\n",
      "\\textbf{Emotion} & 70 & 44 & 1.000 & 0.929 & 0.943 & 0.6233 \\\\\n",
      "\\textbf{Cholecystectomy} & 134 & 92 & 1.000 & 0.851 & 0.902 & 0.4396 \\\\\n",
      "\\textbf{Cardiac} & 66 & 52 & 0.900 & 0.841 & 0.962 & 0.4845 \\\\\n",
      "\\textbf{Sepsis} & 108 & 66 & 0.900 & 0.852 & 0.894 & 0.3500 \\\\\n",
      "\n",
      "\n",
      "---- Claim Decomposition ----\n",
      "Accuracy:0.9428571428571428\n",
      "N:35\n",
      "Kappa:0.7169811320754718\n",
      "\n",
      "---- Relevance Filtering ----\n",
      "Accuracy:0.8711864406779661\n",
      "N:295\n",
      "Kappa:0.4023551259865784\n",
      "\n",
      "---- Expert Alignment ----\n",
      "Accuracy:0.9229857819905213\n",
      "N:211\n",
      "Kappa:0.40516463689670723\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"massmaps\", \"supernova\", \"politeness\", \"emotion\", \"cholec\", \"cardiac\", \"sepsis\"]\n",
    "filepath = \"vanilla_{}_gpt-4o_annot.json\"\n",
    "filepath2 = \"vanilla_{}_gpt-4o_annot2.json\"\n",
    "\n",
    "claim_vals_1 = []\n",
    "claim_vals_2 = []\n",
    "relevance_vals_1 = []\n",
    "relevance_vals_2 = []\n",
    "alignment_vals_1 = []\n",
    "alignment_vals_2 = []\n",
    "\n",
    "dataset_names = {\n",
    "    \"massmaps\": \"Mass Maps\",\n",
    "    \"supernova\": \"Supernova\",\n",
    "    \"politeness\": \"Politeness\",\n",
    "    \"emotion\": \"Emotion\",\n",
    "    \"cholec\": \"Cholecystectomy\",\n",
    "    \"cardiac\": \"Cardiac\",\n",
    "    \"sepsis\": \"Sepsis\"\n",
    "}\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Load the data\n",
    "    try:\n",
    "        with open(filepath.format(dataset), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        with open(filepath2.format(dataset), \"r\") as f:\n",
    "            data2 = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for dataset: {dataset}\")\n",
    "        continue\n",
    "   \n",
    "    keys = data.keys()\n",
    "    claim_accs_all = []\n",
    "    relevance_accs_all = []\n",
    "    alignment_accs_all = []\n",
    "\n",
    "    all_values_1 = []\n",
    "    all_values_2 = []\n",
    "\n",
    "    for key in keys:\n",
    "        #read in annotator 1 examples\n",
    "        example = data[key]\n",
    "        claim_accs = example[\"claim_decomposition_accuracy\"]\n",
    "        relevance_accs = example[\"relevance_filtering_accuracy\"]\n",
    "        relevance_accs = [relevance_accs[x][0] for x in range(len(relevance_accs))]\n",
    "        alignment_accs = example[\"expert_alignment_accuracy\"]\n",
    "        alignment_accs = [alignment_accs[x][0] for x in range(len(alignment_accs))]\n",
    "\n",
    "        claim_vals_1.append(claim_accs)\n",
    "        for r in relevance_accs: relevance_vals_1.append(r)\n",
    "        for a in alignment_accs: alignment_vals_1.append(a)\n",
    "        \n",
    "        #read in annotator 2 examples\n",
    "        example2 = data2[key]\n",
    "        claim_accs2 = example2[\"claim_decomposition_accuracy\"]\n",
    "        relevance_accs2 = example2[\"relevance_filtering_accuracy\"]\n",
    "        relevance_accs2 = [relevance_accs2[x][0] for x in range(len(relevance_accs2))]\n",
    "        alignment_accs2 = example2[\"expert_alignment_accuracy\"]\n",
    "        alignment_accs2 = [alignment_accs2[x][0] for x in range(len(alignment_accs2))]\n",
    "        \n",
    "        claim_vals_2.append(claim_accs2)\n",
    "        for r in relevance_accs2: relevance_vals_2.append(r)\n",
    "        for a in alignment_accs2: alignment_vals_2.append(a)\n",
    "\n",
    "        claim_accs_all.append(claim_accs)\n",
    "        claim_accs_all.append(claim_accs2)\n",
    "        for r in relevance_accs: relevance_accs_all.append(r)\n",
    "        for r in relevance_accs2: relevance_accs_all.append(r)\n",
    "        for a in alignment_accs: alignment_accs_all.append(a)\n",
    "        for a in alignment_accs2: alignment_accs_all.append(a)\n",
    "\n",
    "        all_values_1 += [claim_accs] + relevance_accs + alignment_accs\n",
    "        all_values_2 += [claim_accs2] + relevance_accs2 + alignment_accs2\n",
    "    \n",
    "    # Calculate cohen's kappa\n",
    "    possible_values = [0, 0.5, 1]\n",
    "    assert([x in possible_values for x in all_values_1])\n",
    "    assert([x in possible_values for x in all_values_2])\n",
    "    all_values_1_str = [str(x) for x in all_values_1]\n",
    "    all_values_2_str = [str(x) for x in all_values_2]\n",
    "    kappa = cohen_kappa_score(all_values_1_str, all_values_2_str)\n",
    "\n",
    "    #Print a latex table row\n",
    "    latex_row = (\n",
    "    f\"\\\\textbf{{{dataset_names[dataset]}}} & \"\n",
    "    f\"{len(relevance_accs_all)} & \"\n",
    "    f\"{len(alignment_accs_all)} & \"\n",
    "    f\"{np.mean(claim_accs_all):.3f} & \"\n",
    "    f\"{np.mean(relevance_accs_all):.3f} & \"\n",
    "    f\"{np.mean(alignment_accs_all):.3f} & \"\n",
    "    f\"{kappa:.4f} \\\\\\\\\")\n",
    "\n",
    "    print(latex_row)    \n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "#average for each stage\n",
    "print(\"---- Claim Decomposition ----\")\n",
    "print(\"Accuracy:{}\".format(np.mean(claim_vals_1 + claim_vals_2)))\n",
    "print(\"N:{}\".format(len(claim_vals_1)))\n",
    "claim_vals_1 = [str(x) for x in claim_vals_1]\n",
    "claim_vals_2 = [str(x) for x in claim_vals_2]\n",
    "print(\"Kappa:{}\".format(cohen_kappa_score(claim_vals_1, claim_vals_2)))\n",
    "\n",
    "print(\"\\n---- Relevance Filtering ----\")\n",
    "print(\"Accuracy:{}\".format(np.mean(relevance_vals_1 + relevance_vals_2)))\n",
    "print(\"N:{}\".format(len(relevance_vals_1)))\n",
    "relevance_vals_1 = [str(x) for x in relevance_vals_1]\n",
    "relevance_vals_2 = [str(x) for x in relevance_vals_2]\n",
    "print(\"Kappa:{}\".format(cohen_kappa_score(relevance_vals_1, relevance_vals_2)))\n",
    "\n",
    "print(\"\\n---- Expert Alignment ----\")\n",
    "print(\"Accuracy:{}\".format(np.mean(alignment_vals_1 + alignment_vals_2)))\n",
    "print(\"N:{}\".format(len(alignment_vals_1)))\n",
    "alignment_vals_1 = [str(x) for x in alignment_vals_1]\n",
    "alignment_vals_2 = [str(x) for x in alignment_vals_2]\n",
    "print(\"Kappa:{}\".format(cohen_kappa_score(alignment_vals_1, alignment_vals_2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea7445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
